= AWS 환경에서 구축

본 장에서는 AWS 환경에서 폐쇄망으로 Kubespray를 이용하여 HyperCloud를 설치하는 방법에 대해서 설명한다.

설치하는 과정은 크게 다음과 같다.

. <<TerraformAws, Terraform 구성>>
. <<K8sInfraAws, 쿠버네티스 인프라 구성>>
. <<ArgoCDInstallAws, ArgoCD 설치>>
. <<MasterClusterAws, 마스터 클러스터 설치>>
. <<SingleClusterAws, 싱글 클러스터 설치>>
. <<ResourceDeployAws, 리소스 배포>>

[#TerraformAws]
== Terraform 구성

Terraform을 이용하여 CSP(AWS, vSphere) 환경에서의 쿠버네티스 클러스터 구성을 위한 인프라 리소스를 생성할 수 있다.

.Terraform을 설치하기 위해서는 다음과 같은 사전 준비가 필요하다.
[NOTE]
====
* IAM 계정 발급
* 액세스 키 ID 및 시크릿 키 저장
* 키페어 발급
====

Terraform을 구성하는 순서는 다음과 같다.

. <<TerraformInstallAws, Terraform 설치>>
. <<KubesprayDownTerraform, Kubespray 파일 압축 해제>>
. <<TerraformConfigAws, Terraform 환경 설정>>
. <<TerraformRunAws, Terraform 적용>>


[#TerraformInstallAws]
=== Terraform 설치

외부망 환경에서 Terraform을 사용하기 위해 관련 패키지를 설치한다.

. *저장소 추가*
+
패키지 설치에 사용할 저장소를 추가한다.
+
----
$ sudo yum install -y yum-utils
$ sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo
----

. *패키지 설치*
+
설치 가능한 Terraform의 버전을 확인한다.
+
----
$ yum --showduplicate list terraform
----
+
추가한 저장소에 Terraform을 설치한다.
+
----
$ sudo yum -y install terraform
----

. *설치 확인*
+
설치된 Terraform의 버전을 확인한다.
+
----
$ terraform  version
----

[#KubesprayDownTerraform]
=== Kubespray 파일 압축 해제

쿠버네티스 설치를 위해 테크넷을 통해서 다운로드한 *kubespray-infra.zip* 파일을 압축 해제한다.

압축 해제 후 `kubespray-infra/contrib/terraform/aws/` 경로의 `credentials.tfvars, terraform.tfvars, variables.tf` 파일을 사용한다.

[#TerraformConfigAws]
=== Terraform 환경 설정

`kubespray-infra/contrib/terraform/aws/terraform.tfvars` 파일을 열어 Terraform에 전달하는 변수의 값을 정의한다.

Setting VPN Connection 부분의 경우, 온프레미스 환경과 연결이 필요한 경우 설정한다.
----
#Global Vars
aws_cluster_name = "terraform"

#VPC Vars
aws_vpc_cidr_block       = "10.0.0.0/16"
aws_cidr_subnets_public  = ["10.0.1.0/24"]
aws_cidr_subnets_private = ["10.0.2.0/24"]

#Bastion Host
aws_bastion_num  = 1
aws_bastion_size = "t2.medium" 

#Kubernetes Cluster
aws_kube_master_num  = 3
aws_kube_master_size = "t2.2xlarge"
aws_kube_master_disk_size = 30

aws_etcd_num  = 0
aws_etcd_size = "t2.medium"
aws_etcd_disk_size = 30

aws_kube_worker_num  = 1
aws_kube_worker_size = "t2.2xlarge"
aws_kube_worker_disk_size = 30

#EC2 Source/Dest Check
aws_src_dest_check      = false

#Settings AWS ELB
aws_elb_api_port          = 6443
k8s_secure_api_port       = 6443
aws_elb_api_internal      = true
aws_elb_api_public_subnet = false

default_tags = {
    Env = "terraform-qatest"
    Product = "kubernetes"
    Team = "QA"
}

#Setting VPN Connection

vpn_connection_enable = true
customer_gateway_ip   = "175.195.xxx.xxx"
local_cidr            = "30.0.0.0/16"

inventory_file = "../../../inventory/tmaxcloud/hosts"
----

[#TerraformRunAws]
=== Terraform 적용

변경된 Terraform의 환경 설정을 적용하기 위해 AWS 계정 및 액세스 키 정보를 등록한다.

. *AWS 계정 및 액세스 키 정보 등록*
+
`credentials.tfvars` 파일을 생성한 후 사전에 발급받은 AWS 액세스 키 ID와 보안 액세스 키 정보를 등록한다.
+
.예시
----
#AWS Access Key
AWS_ACCESS_KEY_ID = "AKIAVVIW**********"
#AWS Secret Key
AWS_SECRET_ACCESS_KEY = "oa3ph/GBPkO5Km8rlM*********************"
#EC2 SSH Key Name
AWS_SSH_KEY_NAME = "default"
#AWS Region
AWS_DEFAULT_REGION = "us-east-x"
----

. *Terraform 초기화* 
+
*terraform init* 명령을 수행하여 설정한 Terraform 정보를 저장하고, 필요한 플러그인을 설치한다.
+
----
$ terraform init
----

. *구성할 AWS 리소스 확인* 
+
AWS 액세스 키 ID와 보안 액세스 키 정보가 등록된 `credentials.tfvars` 파일을 *terraform plan* 명령을 사용하여 구성할 AWS 리소스를 확인한다.
+
----
$ terraform plan -var-file=credentials.tfvars
----

. *Terraform을 통한 AWS 리소스 생성* 
+
AWS 액세스 키 ID와 보안 액세스 키 정보가 등록된 `credentials.tfvars` 파일을 *terraform apply* 명령을 사용하여 구성할 적용한다.
+
----
$ terraform apply -var-file=credentials.tfvars
----
+
생성이 완료되면 추후 kubespray를 위해 Terraform 적용 후 생성된 `kubespray-infra/inventory/tmaxcloud/hosts` 파일의 `[etcd]` 부분에 생성된 master ip를 추가한다.
+
----
[etcd]
ip-10-0-2-X.us-east-x.compute.internal
ip-10-0-2-X.us-east-x.compute.internal
ip-10-0-2-X.us-east-x.compute.internal
----

. *생성된 인스턴스와 AWS 리소스 연동을 위한 엔드포인트 생성* 
+
ebs, ec2, efs, elb 연동을 위한 엔드포인트를 각각 생성 한다.
+
아래는 ebs 예시이다.
+
image::../../images/endpoint1.png[]
+
서비스에 각각 ebs, ec2, elasticfilesystem, elasticloadbalancing 을 선택하여 진행한다.
+
image::../../images/endpoint2.png[]
+
서브넷 선택 시 Terraform으로 생성한 VPC 선택 후 private 서브넷만 선택, 보안 그룹은 모두 선택하여 생성한다.


=== Terraform 삭제

Terraform을 통해 구성한 AWS 리소스 전체 삭제 시 아래 내용을 참고한다.

. *AWS 리소스 삭제*
+
Terraform을 통해 만든 리소스만 삭제되므로, 해당 명령어 수행 이후 남아있는 리소스는 수동으로 삭제해야한다.
+
----
$ terraform destroy --var-file=credentials.tfvars
----


[#K8sInfraAws]
== 쿠버네티스 인프라 구성

본 절에서는 Terraform을 통해 생성한 bastion 노드에서 Kubespray를 이용하여 쿠버네티스 인프라를 구성하는 방법에 대해서 설명한다.

쿠버네티스 인프라를 구성하는 순서는 다음과 같다.

. <<KubesprayConfigK8sAws, Kubespray 환경 설정>>
. <<KubesprayRunK8sAws, Kubespray 실행>>

[#KubesprayConfigK8sAws]
=== Kubespray 환경 설정

Kubespray를 실행하기 위한 필수 설정 파일들을 정의한다.

NOTE: Kubespray를 실행하기 위해서는 사전 준비가 필요하다. 반드시  xref:offline-intro.adoc[설치 전 준비사항]을 참고하여 환경을 구성한다. bastion을 proxy하여 master node나 worker node에 접근한다. bastion에도 다른 노드에 접근하기 위해서 pem 파일이 필요하다.

CAUTION: RHEL 운영체제일 경우 `kubespray-infra/cluster.yml` 파일을 열어 *- { role: bootstrap-os, tags: bootstrap-os}* 행을 반드시 주석 처리해야 한다.

. *노드 정보 등록*
+ 
`kubespray-infra/inventory/tmaxcloud/inventory.ini` 파일을 열어 kubespray에서 설치할 노드들의 정보를 등록한다. +
이때 all 그룹은 `*[호스트 이름] [Ansible IP 주소] [Backup IP 주소]*` 형태로 작성하고, 그 외 그룹은 all 그룹에서 정의한 호스트 이름만 작성한다.

. *쿠버네티스 기본 정보 설정*
+
`kubespray-infra/inventory/tmaxcloud/group_vars/all/all.yml` 파일을 열어 Kubernetes의 기본 정보를 설정한다. + 
이때 loadbalancer_apiserver의 address 명은 주석처리 한다.
+
.예시
----
apiserver_loadbalancer_domain_name: "kubernetes-nlb-test-xxx.elb.us-east-x.amazonaws.com" <1>
loadbalancer_apiserver:
# address:
  port: 6443 <2>
  
upstream_dns_servers: <3>
  - /etc/resolv.conf
----
+
<1> AWS ELB(Elastic Load Balancing) 주소
<2> 쿠버네티스 API 서버 포트 번호
<3> AWS 도메인 네임서버 주소

. *폐쇄망 정보 설정*
+
`kubespray-infra/inventory/tmaxcloud/group_vars/all/offline.yml` 파일을 열어 폐쇄망 관련 정보를 설정한다.
+
.예시
----
is_this_offline: true <1>
registry_host: "10.0.10.50:5000" <2>
files_repo: "http://172.22.5.2" <3>
----
+
<1> 폐쇄망 환경 여부 (폐쇄망일 경우 true)
<2> 프라이빗 레지스트리 주소
<3> 파일 리포지터리 주소

. *Calico 구성 정보 설정*
+
`kubespray-infra/inventory/tmaxcloud/group_vars/k8s_cluster/k8s-net-calico.yml` 파일을 열어 Calico 관련 정보를 설정한다.
+
.예시
----
calico_ipv4pool_ipip: "Always" <1>
calico_ipip_mode: "Always" <2>
calico_ip_auto_method: "cidr=20.0.6.0/24, 20.0.7.0/24, 20.0.8.0/24" <3>
----
+
<1> 기본 ippool의 IP-in-IP 모드 활성화 여부
<2> Calico의 IP-in-IP 모드 활성화 여부
<3> Calico가 자동으로 감지할 노드들의 CIDR 값 

. *IP 주소 대역 설정*
+
`kubespray-infra/inventory/tmaxcloud/group_vars/k8s_cluster/k8s-cluster.yml` 파일을 열어 파드 및 서비스의 IP 주소 대역 정보를 설정한다.
+
.예시
----
# Kubernetes internal network for services, unused block of space.
kube_service_addresses: 10.96.0.0/24 <1>

# internal network. When used, it will assign IP
# addresses from this range to individual pods.
# This network must be unused in your network infrastructure!
kube_pods_subnet: 10.244.0.0/24 <2>
----
+
<1> 서비스 IP 주소 대역
<2> 파드 서브넷 IP 주소 대역

. *추가 설치 모듈 설정*
+
`kubespray-infra/inventory/tmaxcloud/group_vars/k8s_cluster/addons.yml` 파일을 열어 추가 설치가 가능한 모듈 관련 정보를 설정한다.
+
.예시
----
default_storageclass_name: efs-sc <1>
sc_name_0: efs-sc-0 <2>
sc_name_999: efs-sc-999 <3>
nfs_external_provisioner_enabled: false <4>
aws_efs_csi_enabled: true <5>
aws_efs_csi_namespace: aws-efs-csi <6>
aws_efs_filesystem_id: fs-XXX <7>
----
+
<1> 기본값으로 설정할 스토리지 이름
<2> HyperRegistry에서 Postgres PVC의 스토리지 클래스 이름
<3> 그 외의 PVC 스토리지 클래스 이름
<4> NFS Provisioner 설치 여부
<5> AWS EFS CSI 드라이버 설치 여부
<6> AWS EFS CSI 스토리지의 네임스페이스 이름
<7> AWS EFS 파일 시스템의 ID

[#KubesprayRunK8sAws]
=== Kubespray 실행

ansible-playbook 명령을 사용하여 Kubespray를 실행한다.

.사용 방법
----
$ ansible-playbook -i ./inventory/tmaxcloud/hosts ./cluster.yml -e ansible_user=ec2-user -e bootstrap_os=redhat -e ansible_ssh_private_key_file={PEM_PATH} -e cloud_provider=aws -b --become-user=root --flush-cache -v
----
Kubespray 실행 명령의 인자 값에 대한 설명은 다음과 같다.

[width="100%",options="header", cols="1,3"]
|====================
|인자 값|설명
|{PEM_PATH}|다운로드한 PEM 파일의 경로 (예: /root/default.pem)
|====================

=== Kubernetes config

Master Node에서 아래 명령어를 실행한다.

----
$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config
----

=== Kubespray Infra 삭제

Kubespray를 통해 구성한 k8s 리소스 전체 삭제 시 아래 내용을 참고한다.

*k8s 리소스 삭제*
----
$ ansible-playbook -i ./inventory/tmaxcloud/hosts ./reset.yml -e ansible_user=ec2-user -e bootstrap_os=redhat -e ansible_ssh_private_key_file={PEM_PATH} -e cloud_provider=aws -b --become-user=root --flush-cache -v
----

[#ArgoCDInstallAws]
== ArgoCD 설치

본 절에서는 Terraform을 통해 생성한 bastion 노드에서 Kubespray를 이용하여 ArgoCD를 설치하는 방법에 대해서 설명한다.

ArgoCD를 설치하는 순서는 다음과 같다.

. <<KubesprayDecompressionArgoAws, Kubespray 파일 압축 해제>>
. <<KubesprayConfigArgoAws, Kubespray 환경 설정>>
. <<KubesprayRunArgoAws, Kubespray 실행>>

[#KubesprayDecompressionArgoAws]
=== Kubespray 파일 압축 해제

ArgoCD 설치를 위해 테크넷을 통해서 다운로드한 *kubespray-aws.zip* 파일의 압축을 해제한다.

[#KubesprayConfigArgoAws]
=== Kubespray 환경 설정

Kubespray를 실행하기 위한 필수 설정 파일들을 정의한다.

CAUTION: RHEL 운영체제일 경우 `kubespray-aws/cluster.yml` 파일을 열어 *- { role: bootstrap-os, tags: bootstrap-os}* 행을 반드시 주석 처리해야 한다.

. *노드 정보 등록*
+ 
`kubespray-aws/inventory/tmaxcloud/inventory.ini` 파일을 열어 kubespray에서 설치할 노드들의 정보를 등록한다. +
이때 all 그룹은 `*[호스트 이름] [Ansible IP 주소] [Backup IP 주소]*` 형태로 작성하고, 그 외 그룹은 all 그룹에서 정의한 호스트 이름만 작성한다.

. *폐쇄망 정보 설정*
+
`kubespray-aws/inventory/tmaxcloud/group_vars/all/offline.yml` 파일을 열어 폐쇄망 관련 정보를 설정한다.
+
.예시
----
is_this_offline: true <1>
registry_host: "10.0.10.50:5000" <2>
files_repo: "http://172.22.5.2" <3>
----
+
<1> 폐쇄망 환경 여부 (폐쇄망일 경우 true)
<2> 프라이빗 레지스트리 주소
<3> 파일 리포지터리 주소

. *사용자 지정 도메인 등록*
+
`kubespray-aws/inventory/tmaxcloud/group_vars/k8s_cluster/k8s-cluster.yml` 파일을 열어 외부에 노출할 사용자 지정 도메인의 정보를 등록한다.
+
.예시
----
# Enable extra custom DNS domain - by sophal_hong@tmax.co.kr
enable_local_nip_domain: false <1>
enable_custom_domain: true <2>
custom_domain_name: "cloudqa.com" <3>
custom_domain_ip: 172.22.7.2 <4>
api_server_dns_cfwhn: true <5>

# Kubernetes internal network for services, unused block of space.
kube_service_addresses: 10.96.0.0/24 <6>

# internal network. When used, it will assign IP
# addresses from this range to individual pods.
# This network must be unused in your network infrastructure!
kube_pods_subnet: 10.244.0.0/24 <7>
----
+
<1> nip.io 도메인의 사용 여부 (Self-Signed 도메인을 사용할 경우 true)
<2> 커스텀 도메인의 사용 여부 (DNS를 사용할 경우 true)
<3> 프록시 노드에 맵핑된 DNS 이름
<4> 프록시 노드의 IP 주소 
<5> kube-apiserver의 DNS 정책으로 "ClusterFirstWithHostNet" 적용 여부 
<6> 서비스 IP 주소 대역
<7> 파드 서브넷 IP 주소 대역

. *설치할 애플리케이션 구성 정보 확인*
+
Kubespray로 설치될 애플리케이션(`nginx`, `hyperregistry`, `gitea`, `argocd`)의 구성 정보를 확인 및 설정한다. +
해당 애플리케이션의 구성 정보는 기본적으로 `kubespray-aws/inventory/tmaxcloud/group_vars/k8s_cluster/addon.yml` 파일에서 설정이 가능하며, 추가적으로 커스터마이징이 필요할 경우에는 `kubespray-aws/roles/bootstrap-cloud/task/` 및 `kubespray-aws/roles/bootstrap-cloud/templates/` 하위 파일에서 설정이 가능하다.
+

[#KubesprayRunArgoAws]
=== Kubespray 실행

ansible-playbook 명령을 사용하여 애플리케이션을 설치한다.

.사용 방법
----
$ ansible-playbook -i ./inventory/tmaxcloud/inventory.ini ./cluster.yml -t bootstrap-cloud -e ansible_user=centos -e ansible_ssh_private_key_file={PEM_PATH} -e cloud_provider=aws -b --become-user=root
----
Kubespray 실행 명령의 인자 값에 대한 설명은 다음과 같다.

[width="100%",options="header", cols="1,3"]
|====================
|인자 값|설명
|{PEM_PATH}|다운로드한 PEM 파일의 경로 (예: /root/default.pem)
|====================

애플리케이션 설치가 정상적으로 완료되면, Gitea과 ArgoCD 간의 저장소가 자동으로 연동된다.

CAUTION: Selfsigned 인증서로 Gitea를 설치했을 경우, kubespray-onpremise/roles/bootstrap-cloud/tasks/install-argocd.yml 에서 Gitea 주소를 적용하는 task 주석을 제거한다.
----
- name: ArgoCD | Apply Secret for self-signed Gitea
  ansible.builtin.shell: |
    sudo chmod +755 ./argocd-apply-gitea-repo-secret.sh
    sudo ./argocd-apply-gitea-repo-secret.sh
  args:
    chdir: "{{ kube_config_dir }}/addons/argocd"
  ignore_errors: true
----
====

[#MasterClusterAws]
== 마스터 클러스터 설치

. *master-values.yaml 파일 gateway 수정*
+
`kubespray-onpremise/roles/bootstrap-cloud/templates/argocd_installer/application/helm/master-values.yaml` 파일을 열어 gateway 설정, 각 앱들의 사용 여부, 로그 레벨 및 세부 환경 변수를 정의한다.
+
.예시
----
...
  gatewayBootstrap:
    enabled: true
    ...
    service:
      type: LoadBalancer <1>
    tls:
      selfsigned:
        enabled: true <2>
      acme:
        enabled: false <3>
        email: test@tmax.co.kr
        dns:
          type: route53
          accessKeyID: accesskey <4>
          accessKeySecret: secretkey <5>
          hostedZoneID: hostedzoneid <6>
        environment: production <7>
----
<1> 네트워크 서비스 타입
* LoadBalancer
* NodePort
<2> 자체 서명 인증서의 사용 여부
<3> Route 53으로 생성한 도메인을 사용하기 위한 자동 인증서 관리 환경 사용 여부
<4> AWS에서 사용하는 계정의 액세스 키 ID
<5> 액세스 키 ID에 대한 시크릿 키
<6> Route 53으로 생성한 도메인에 대한 호스팅 영역 ID
<7> 실제 사용할 인증서 발급 용도
* 운영용 : production
* 테스트용 : staging
+
. *멀티클러스터를 사용할 경우 master-values.yaml 파일 cluster-api 수정*
+
.예시
----
### cluster-api
  capi:
    ...
    providers:
      aws:
        enabled: true <1>
        credentials:
          accessKeyID: access-key <2>
          secretAccessKeyID: secret-access-key <3>
      ...
      vsphere:
        enabled: true <4>
        credentials:
          username: "user" <5>
          password: "password" <6>
----
<1> capa 설치 여부
<2> 멀티클러스터용 AWS에서 사용하는 계정의 액세스 키 ID
<3> 액세스 키 ID에 대한 시크릿 키
<4> capv 설치 여부
<5> vcenter ID
<6> vcenter Password

+
NOTE: 예시 외에 설치할 모듈에 대한 enabled 값을 true로 설정하거나, 필요시 사용자 지정 도메인을 등록한다.

. *애플리케이션 변수 설정*
+
`kubespray-onpremise/roles/bootstrap-cloud/templates/argocd_installer/application/app_of_apps/master-applications.yaml` 파일을 열어 마스터 클러스터의 애플리케이션 변수를 설정한다.
+
.예시
----
source:
      ...
      parameters:
        - name: global.domain
          value: "글로벌 도메인을 입력하세요 ex) testdomain.com" <1>
        - name: global.masterSingle.hyperAuthDomain
          value: "hyperauth full 도메인을 입력하세요 ex) hyperauth.testdomain.com" <2>
        # Avaliable values: UTC, Asia/Seoul
        - name: global.timeZone
          value: "UTC" <3>
        - name: global.network.disabled
          value: "true" <4>
        - name: global.privateRegistry
          value: "폐쇄망일 경우 image registry 주소를 입력하세요 ex) https://hyperregistry.testdomain.com" <5>
        - name: spec.source.repoURL
          value: "git repository URL을 입력하세요 ex) https://github.com/tmax-cloud/argocd-installer.git" <6>
        - name: spec.source.targetRevision
          value: "target Revision을 입력하세요 ex) main" <7>
    path: application/helm
    # 환경에 맞게 url 주소 변경 필요
    repoURL: https://github.com/tmax-cloud/argocd-installer <8>
    # 환경에 맞게 target branch/release 변경 필요
    targetRevision: HEAD <9>
----
<1> 애플리케이션 설치 시 인그레스 주소에 사용될 커스텀 도메인 이름
<2> 마스터 클러스터와 싱글 클러스터에서 사용할 HyperAuth 주소
<3> 애플리케이션 타임존 설정 
* UTC
* Asia/Seoul
<4> 폐쇄망 환경 여부 (폐쇄망일 경우 true)
<5> 프라이빗 컨테이너 이미지 레지스트리의 주소
<6> 최상위 변수용 ArgoCD와 연동된 Gitea 저장소 주소 (Gitea의 경우 URL 마지막에 .git을 추가)
<7> 최상위 변수용 Gitea에 연동되어 있는 argocd-installer의 브랜치 이름
<8> master-applications.yaml용 ArgoCD와 연동된 Gitea 저장소 주소 (Gitea의 경우 URL 마지막에 .git을 추가)
<9> master-applications.yaml용 Gitea에 연동되어 있는 argocd-installer의 브랜치 이름

. *Gitea 동기화 작업*
+
ArgoCD와 연동된 Gitea의 argocd-installer 브랜치에서 `master-values.yaml`, `master-applications.yaml` 파일을 열어 위의 1~3번 과정과 동일하게 환경 변수를 설정한다.

. *애플리케이션 등록*
+
설치 환경에 애플리케이션을 등록한다.
+
----
$ kubectl -n argocd apply -f application/app_of_apps/master-applications.yaml
----

[#SingleClusterAws]
== 싱글 클러스터 설치

. *생성된 애플리케이션 파일 불러오기*
+
HyperCloud 웹 콘솔의 "멀티 클러스터" 콘솔에서 *[클러스터]* 메뉴를 클릭하면 싱글 클러스터 목록이 조회된다. 이때 싱글 클러스터 상태가 "Sync Needed"로 변경되면 해당 상태를 클릭한다.
+
image::../../images/figure_single_cluser_install_01.png[]

. *애플리케이션 변수 설정*
+
싱글 클러스터의 애플리케이션 동기화 옵션 설정 화면이 열리면 "REPO URL"과 "TARGET REVISION" 항목을 설정한다.
+
image::../../images/figure_single_cluser_install_03.png[]
+
설정 완료 후 애플리케이션의 *[SYNC]* 버튼을 클릭한다
+
image::../../images/figure_single_cluser_install_02.png[]

CAUTION: 클러스터 클레임으로 생성한 클러스터를 배포할 경우 대시보드를 통한 리소스 사용량 조회가 일부 지원되지 않는다.

[#ResourceDeployAws]
== 리소스 배포

애플리케이션 동기화 작업을 통해 리소스를 배포한다.

이때 마스터 클러스터와 싱글 클러스터에서 각각 동기화 작업을 진행해야 하며, 각 애플리케이션의 동기화 순서는 아래를 참고한다.

[CAUTION]
.마스터 클러스터 동기화 순서
====
마스터 클러스터에서 애플리케이션 동기화 순서는 다음과 같다. 반드시 순서에 맞게 동기화 작업을 수행한다. +
1. cert-manager, api-gateway, console, (multi cluster 생성 시 jwt-decode-auth) +
2. strimzi-kafka-operator + hyperauth +
3. oauth2-proxy +
4. gitea, argocd, hyperregistry +
5. prometheus +
6. loki + 
7. grafana-operator +
8. service-mesh(istio, jaeger, kiali) +
9. hypercloud +
10. template-service-broker +
11. catalog-controller +
12. cicd-operator(tekton) +
13. ai-devops + 
14. cluster-api(aws, vsphere) +
15. redis-operator, helm-apiserver, service-binding-operator +
16. sonarqube, nexus +
17. image-validating-webhook
====

[CAUTION]
.싱글 클러스터 동기화 순서
====
싱글 클러스터에서 애플리케이션 동기화 순서는 다음과 같다. 반드시 순서에 맞게 동기화 작업을 수행한다. +
1. cert-manager, api-gateway, console +
2. prometheus +
3. loki + 
4. grafana-operator +
5. hyperregistry +
6. service-mesh(istio, jaeger, kiali) + 
7. template-service-broker +
8. catalog-controller, cicd-operator(tekton), redis-operator, helm-apiserver, service-binding-operator +
9. ai-devops, sonarqube, nexus +
10. image-validating-webhook
====

. *ArgoCD 콘솔 접속*
+
웹 브라우저의 주소 표시줄에 ArgoCD 서버의 주소를 입력한다.
+
[NOTE]
====
ArgoCD 서버 주소는 다음의 명령을 실행하여 확인할 수 있다.
----
$ kubectl get ingress -n argocd
----
====

. *ArgoCD 콘솔 로그인*
+
ArgoCD 콘솔 로그인 화면이 열리면 계정 아이디와 비밀번호를 입력한 후 *[SIGN IN]* 버튼을 클릭한다.
+
[NOTE]
====
ArgoCD 계정 아이디 및 초기 비밀번호 정보는 다음의 명령을 실행하여 확인할 수 있다.
----
$ kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d; echo
----
ArgoCD 콘솔에 첫 로그인 시 위에서 확인한 계정 정보로 로그인이 가능하며, 로그인 후 *[User Info]* 메뉴를 통해 비밀번호를 변경할 수 있다.
====

. *동기화할 애플리케이션 검색*
+
ArgoCD 콘솔의 **Applications 화면**에서 동기화 작업을 수행할 애플리케이션을 검색한 후 *[SYNC]* 버튼을 클릭한다.
+
image::../../images/figure_application_sync_01.png[]

. *동기화 옵션 설정*
+
동기화할 리소스 및 동기화 옵션을 설정한 후 *[SYNCHRONIZE]* 버튼을 클릭한다.
+
image::../../images/figure_application_sync_02.png[]

. *상태 확인*
+
애플리케이션의 *Status* 항목에 "Healthy"와 "Synced"가 표시되는지 확인한다.
+
image::../../images/figure_application_sync_03.png[]
