= AWS 환경에서 구축

본 장에서는 AWS 환경에서 폐쇄망으로 Kubespray를 이용하여 HyperCloud를 설치하는 방법에 대해서 설명한다.

설치하는 과정은 크게 다음과 같다.

. <<TerraformAws, Terraform 구성>>
. <<K8sInfraAws, 쿠버네티스 인프라 구성>>
. <<ArgoCDInstallAws, ArgoCD 설치>>
. <<MasterClusterAws, 마스터 클러스터 설치>>
. <<SingleClusterAws, 싱글 클러스터 설치>>
. <<ResourceDeployAws, 리소스 배포>>

[#TerraformAws]
== Terraform 구성

Terraform을 이용하여 CSP(AWS, vSphere) 환경에서의 쿠버네티스 클러스터 구성을 위한 인프라 리소스를 생성할 수 있다.

.Terraform을 설치하기 위해서는 다음과 같은 사전 준비가 필요하다.
[NOTE]
====
* IAM 계정 발급
* 액세스 키 ID 및 시크릿 키 저장
* 키페어 발급
====

Terraform을 구성하는 순서는 다음과 같다.

. <<TerraformInstallAws, Terraform 설치>>
. <<KubesprayDownTerraform, Kubespray 파일 압축 해제>>
. <<TerraformConfigAws, Terraform 환경 설정>>
. <<TerraformRunAws, Terraform 적용>>


[#TerraformInstallAws]
=== Terraform 설치

외부망 환경에서 Terraform을 사용하기 위해 관련 패키지를 설치한다.

. *저장소 추가*
+
패키지 설치에 사용할 저장소를 추가한다.
+
----
$ sudo yum install -y yum-utils
$ sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo
----

. *패키지 설치*
+
설치 가능한 Terraform의 버전을 확인한다.
+
----
$ yum --showduplicate list terraform
----
+
추가한 저장소에 Terraform을 설치한다.
+
----
$ sudo yum -y install terraform
----

. *설치 확인*
+
설치된 Terraform의 버전을 확인한다.
+
----
$ terraform  version
----

[#KubesprayDownTerraform]
=== Kubespray 파일 압축 해제

쿠버네티스 설치를 위해 테크넷을 통해서 다운로드한 *kubespray-infra.zip* 파일을 압축 해제한다.

압축 해제 후 `kubespray-infra/contrib/terraform/aws/` 경로의 `credentials.tfvars, terraform.tfvars, variables.tf` 파일을 사용한다.

[#TerraformConfigAws]
=== Terraform 환경 설정

`kubespray-infra/contrib/terraform/aws/terraform.tfvars` 파일을 열어 Terraform에 전달하는 변수의 값을 정의한다.

Setting VPN Connection 부분의 경우, 온프레미스 환경과 연결이 필요한 경우 설정한다.
----
#Global Vars
aws_cluster_name = "terraform"

#VPC Vars
aws_vpc_cidr_block       = "10.0.0.0/16"
aws_cidr_subnets_public  = ["10.0.1.0/24"]
aws_cidr_subnets_private = ["10.0.2.0/24"]

#Bastion Host
aws_bastion_num  = 1
aws_bastion_size = "t2.medium" 

#Kubernetes Cluster
aws_kube_master_num  = 3
aws_kube_master_size = "t2.2xlarge"
aws_kube_master_disk_size = 30

aws_etcd_num  = 0
aws_etcd_size = "t2.medium"
aws_etcd_disk_size = 30

aws_kube_worker_num  = 1
aws_kube_worker_size = "t2.2xlarge"
aws_kube_worker_disk_size = 30

#EC2 Source/Dest Check
aws_src_dest_check      = false

#Settings AWS ELB
aws_elb_api_port          = 6443
k8s_secure_api_port       = 6443
aws_elb_api_internal      = true
aws_elb_api_public_subnet = false

default_tags = {
    Env = "terraform-qatest"
    Product = "kubernetes"
    Team = "QA"
}

#Setting VPN Connection

vpn_connection_enable = true
customer_gateway_ip   = "175.195.xxx.xxx"
local_cidr            = "30.0.0.0/16"

inventory_file = "../../../inventory/tmaxcloud/hosts"
----

[#TerraformRunAws]
=== Terraform 적용

변경된 Terraform의 환경 설정을 적용하기 위해 AWS 계정 및 액세스 키 정보를 등록한다.

. *AWS 계정 및 액세스 키 정보 등록*
+
`credentials.tfvars` 파일을 생성한 후 사전에 발급받은 AWS 액세스 키 ID와 보안 액세스 키 정보를 등록한다.
+
.예시
----
#AWS Access Key
AWS_ACCESS_KEY_ID = "AKIAVVIW**********"
#AWS Secret Key
AWS_SECRET_ACCESS_KEY = "oa3ph/GBPkO5Km8rlM*********************"
#EC2 SSH Key Name
AWS_SSH_KEY_NAME = "default"
#AWS Region
AWS_DEFAULT_REGION = "us-east-x"
----

. *Terraform 초기화* 
+
*terraform init* 명령을 수행하여 설정한 Terraform 정보를 저장하고, 필요한 플러그인을 설치한다.
+
----
$ terraform init
----

. *구성할 AWS 리소스 확인* 
+
AWS 액세스 키 ID와 보안 액세스 키 정보가 등록된 `credentials.tfvars` 파일을 *terraform plan* 명령을 사용하여 구성할 AWS 리소스를 확인한다.
+
----
$ terraform plan -var-file=credentials.tfvars
----

. *Terraform을 통한 AWS 리소스 생성* 
+
AWS 액세스 키 ID와 보안 액세스 키 정보가 등록된 `credentials.tfvars` 파일을 *terraform apply* 명령을 사용하여 구성할 적용한다.
+
----
$ terraform apply -var-file=credentials.tfvars
----
+
생성이 완료되면 추후 kubespray를 위해 Terraform 적용 후 생성된 `kubespray-infra/inventory/tmaxcloud/hosts` 파일의 `[etcd]` 부분에 생성된 master ip를 추가한다.
+
----
[etcd]
ip-10-0-2-X.us-east-x.compute.internal
ip-10-0-2-X.us-east-x.compute.internal
ip-10-0-2-X.us-east-x.compute.internal
----


=== Terraform 삭제

Terraform을 통해 구성한 AWS 리소스 전체 삭제 시 아래 내용을 참고한다.

. *AWS 리소스 삭제*
+
Terraform을 통해 만든 리소스만 삭제되므로, 해당 명령어 수행 이후 남아있는 리소스는 수동으로 삭제해야한다.
+
----
$ terraform destroy --var-file=credentials.tfvars
----


[#K8sInfraAws]
== 쿠버네티스 인프라 구성

본 절에서는 Terraform을 통해 생성한 bastion 노드에서 Kubespray를 이용하여 쿠버네티스 인프라를 구성하는 방법에 대해서 설명한다.

쿠버네티스 인프라를 구성하는 순서는 다음과 같다.

. <<KubesprayConfigK8sAws, Kubespray 환경 설정>>
. <<KubesprayRunK8sAws, Kubespray 실행>>

[#KubesprayConfigK8sAws]
=== Kubespray 환경 설정

Kubespray를 실행하기 위한 필수 설정 파일들을 정의한다.

NOTE: Kubespray를 실행하기 위해서는 사전 준비가 필요하다. 반드시  xref:offline-intro.adoc[설치 전 준비사항]을 참고하여 환경을 구성한다. 

CAUTION: RHEL 운영체제일 경우 `kubespray-infra/cluster.yml` 파일을 열어 *- { role: bootstrap-os, tags: bootstrap-os}* 행을 반드시 주석 처리해야 한다.

. *노드 정보 등록*
+ 
`kubespray-infra/inventory/tmaxcloud/inventory.ini` 파일을 열어 kubespray에서 설치할 노드들의 정보를 등록한다. +
이때 all 그룹은 `*[호스트 이름] [Ansible IP 주소] [Backup IP 주소]*` 형태로 작성하고, 그 외 그룹은 all 그룹에서 정의한 호스트 이름만 작성한다.

. *쿠버네티스 기본 정보 설정*
+
`kubespray-infra/inventory/tmaxcloud/group_vars/all/all.yml` 파일을 열어 Kubernetes의 기본 정보를 설정한다.
+
.예시
----
apiserver_loadbalancer_domain_name: "kubernetes-nlb-test-xxx.elb.us-east-x.amazonaws.com" <1>
loadbalancer_apiserver:
  port: 6443 <2>
  
upstream_dns_servers: <3>
  - 20.0.0.2
----
+
<1> AWS ELB(Elastic Load Balancing) 주소
<2> 쿠버네티스 API 서버 포트 번호
<3> AWS 도메인 네임서버 주소
 
. *Calico 구성 정보 설정*
+
`kubespray-infra/inventory/tmaxcloud/group_vars/k8s_cluster/k8s-net-calico.yml` 파일을 열어 Calico 관련 정보를 설정한다.
+
.예시
----
calico_ipip_mode: "Always" <1>
calico_ip_auto_method: "cidr=20.0.6.0/24, 20.0.7.0/24, 20.0.8.0/24" <2>
----
+
<1> Calico의 IP-in-IP 모드 활성화 여부
<2> Calico가 자동으로 감지할 노드들의 CIDR 값 

. *추가 설치 모듈 설정*
+
`kubespray-infra/inventory/tmaxcloud/group_vars/k8s_cluster/addons.yml` 파일을 열어 추가 설치가 가능한 모듈 관련 정보를 설정한다.
+
.예시
----
default_storageclass_name: efs-sc <1>
sc_name_0: efs-sc-0 <2>
sc_name_999: efs-sc-999 <3>
nfs_external_provisioner_enabled: false <4>
aws_efs_csi_enabled: true <5>
aws_efs_csi_namespace: aws-efs-csi <6>
aws_efs_filesystem_id: fs-XXX <7>
----
+
<1> 기본값으로 설정할 스토리지 이름
<2> HyperRegistry에서 Postgres PVC의 스토리지 클래스 이름
<3> 그 외의 PVC 스토리지 클래스 이름
<4> NFS Provisioner 설치 여부
<5> AWS EFS CSI 드라이버 설치 여부
<6> AWS EFS CSI 스토리지의 네임스페이스 이름
<7> AWS EFS 파일 시스템의 ID

. *폐쇄망 정보 설정*
+
`kubespray-infra/inventory/tmaxcloud/group_vars/all/offline.yml` 파일을 열어 폐쇄망 관련 정보를 설정한다.
+
.예시
----
is_this_offline: true <1>
registry_host: "10.0.10.50:5000" <2>
files_repo: "http://172.22.5.2" <3>
----
+
<1> 폐쇄망 환경 여부 (폐쇄망일 경우 true)
<2> 프라이빗 레지스트리 주소
<3> 파일 리포지터리 주소

. *IP 주소 대역 설정*
+
`kubespray-infra/inventory/tmaxcloud/group_vars/k8s_cluster/k8s-cluster.yml` 파일을 열어 파드 및 서비스의 IP 주소 대역 정보를 설정한다.
+
.예시
----
# Kubernetes internal network for services, unused block of space.
kube_service_addresses: 10.96.0.0/24 <1>

# internal network. When used, it will assign IP
# addresses from this range to individual pods.
# This network must be unused in your network infrastructure!
kube_pods_subnet: 10.244.0.0/24 <2>
----
+
<1> 서비스 IP 주소 대역
<2> 파드 서브넷 IP 주소 대역

[#KubesprayRunK8sAws]
=== Kubespray 실행

ansible-playbook 명령을 사용하여 Kubespray를 실행한다.

.사용 방법
----
$ ansible-playbook -i ./inventory/tmaxcloud/inventory.ini ./cluster.yml -e ansible_user=centos -e ansible_ssh_private_key_file={PEM_PATH} -e cloud_provider=aws -b --become-user=root --flush-cache -v
----
Kubespray 실행 명령의 인자 값에 대한 설명은 다음과 같다.

[width="100%",options="header", cols="1,3"]
|====================
|인자 값|설명
|{PEM_PATH}|다운로드한 PEM 파일의 경로 (예: /root/default.pem)
|====================

[#ArgoCDInstallAws]
== ArgoCD 설치

본 절에서는 Terraform을 통해 생성한 bastion 노드에서 Kubespray를 이용하여 ArgoCD를 설치하는 방법에 대해서 설명한다.

ArgoCD를 설치하는 순서는 다음과 같다.

. <<KubesprayDecompressionArgoAws, Kubespray 파일 압축 해제>>
. <<KubesprayConfigArgoAws, Kubespray 환경 설정>>
. <<KubesprayRunArgoAws, Kubespray 실행>>

[#KubesprayDecompressionArgoAws]
=== Kubespray 파일 압축 해제

ArgoCD 설치를 위해 테크넷을 통해서 다운로드한 *kubespray-aws.zip* 파일의 압축을 해제한다.

[#KubesprayConfigArgoAws]
=== Kubespray 환경 설정

Kubespray를 실행하기 위한 필수 설정 파일들을 정의한다.

CAUTION: RHEL 운영체제일 경우 `kubespray-aws/cluster.yml` 파일을 열어 *- { role: bootstrap-os, tags: bootstrap-os}* 행을 반드시 주석 처리해야 한다.

. *노드 정보 등록*
+ 
`kubespray-aws/inventory/tmaxcloud/inventory.ini` 파일을 열어 kubespray에서 설치할 노드들의 정보를 등록한다. +
이때 all 그룹은 `*[호스트 이름] [Ansible IP 주소] [Backup IP 주소]*` 형태로 작성하고, 그 외 그룹은 all 그룹에서 정의한 호스트 이름만 작성한다.

. *폐쇄망 정보 설정*
+
`kubespray-aws/inventory/tmaxcloud/group_vars/all/offline.yml` 파일을 열어 폐쇄망 관련 정보를 설정한다.
+
.예시
----
is_this_offline: true <1>
registry_host: "10.0.10.50:5000" <2>
files_repo: "http://172.22.5.2" <3>
----
+
<1> 폐쇄망 환경 여부 (폐쇄망일 경우 true)
<2> 프라이빗 레지스트리 주소
<3> 파일 리포지터리 주소

. *사용자 지정 도메인 등록*
+
`kubespray-aws/inventory/tmaxcloud/group_vars/k8s_cluster/k8s-cluster.yml` 파일을 열어 외부에 노출할 사용자 지정 도메인의 정보를 등록한다.
+
.예시
----
# Enable extra custom DNS domain - by sophal_hong@tmax.co.kr
enable_local_nip_domain: false <1>
enable_custom_domain: true <2>
custom_domain_name: "cloudqa.com" <3>
custom_domain_ip: 172.22.7.2 <4>
api_server_dns_cfwhn: true <5>

# Kubernetes internal network for services, unused block of space.
kube_service_addresses: 10.96.0.0/24 <6>

# internal network. When used, it will assign IP
# addresses from this range to individual pods.
# This network must be unused in your network infrastructure!
kube_pods_subnet: 10.244.0.0/24 <7>
----
+
<1> nip.io 도메인의 사용 여부 (Self-Signed 도메인을 사용할 경우 true)
<2> 커스텀 도메인의 사용 여부 (DNS를 사용할 경우 true)
<3> 프록시 노드에 맵핑된 DNS 이름
<4> 프록시 노드의 IP 주소 
<5> kube-apiserver의 DNS 정책으로 "ClusterFirstWithHostNet" 적용 여부 
<6> 서비스 IP 주소 대역
<7> 파드 서브넷 IP 주소 대역

. *설치할 애플리케이션 구성 정보 확인*
+
Kubespray로 설치될 애플리케이션(`nginx`, `hyperregistry`, `gitea`, `argocd`)의 구성 정보를 확인 및 설정한다. +
해당 애플리케이션의 구성 정보는 기본적으로 `kubespray-aws/inventory/tmaxcloud/group_vars/k8s_cluster/addon.yml` 파일에서 설정이 가능하며, 추가적으로 커스터마이징이 필요할 경우에는 `kubespray-aws/roles/bootstrap-cloud/task/` 및 `kubespray-aws/roles/bootstrap-cloud/templates/` 하위 파일에서 설정이 가능하다.
+
AWS 환경에서는 AWS ELB(Elastic Load Balancing)를 사용하기 위해 다음과 같이 인그레스의 서비스 타입을 "LoadBalancer"로 설정해야 한다.
+
.kubespray-aws/roles/bootstrap-cloud/defaults/main.yml
----
ingress_nginx_service_type: LoadBalancer
----

[#KubesprayRunArgoAws]
=== Kubespray 실행

ansible-playbook 명령을 사용하여 애플리케이션을 설치한다.

.사용 방법
----
$ ansible-playbook -i ./inventory/tmaxcloud/inventory.ini ./cluster.yml -t bootstrap-cloud -e ansible_user=centos -e ansible_ssh_private_key_file={PEM_PATH} -e cloud_provider=aws -b --become-user=root --flush-cache -v
----
Kubespray 실행 명령의 인자 값에 대한 설명은 다음과 같다.

[width="100%",options="header", cols="1,3"]
|====================
|인자 값|설명
|{PEM_PATH}|다운로드한 PEM 파일의 경로 (예: /root/default.pem)
|====================

NOTE: 애플리케이션 설치가 정상적으로 완료되면, Gitea과 ArgoCD 간의 저장소가 자동으로 연동된다.

[#MasterClusterAws]
== 마스터 클러스터 설치

. *master-values.yaml 파일 수정*
+
`kubespray-aws/roles/bootstrap-cloud/templates/argocd_installer/application/helm/master-values.yaml` 파일을 열어 애플리케이션을 Helm Chart로 설치하기 위해 사용할 환경 변수를 정의한다.
+
.예시
----
...
global:
  privateRegistry: 10.0.0.1:5000 <1>
...
  gatewayBootstrap:
    enabled: true <2>
    svc_type: LoadBalancer <3>
    tls:
      selfsigned:
        enabled: false <4>
      acme:
        enabled: true <5>
        email: test@tmax.co.kr
        dns:
          type: route53
          accessKeyID: AKIAVXXXXXXXXXX <6>
          accessKeySecret: kFOYY4dYyXXXXXXXXXXXXXXXXXXXX <7>
          hostedZoneID: Z077XXXXXXXXXXXX <8>
        environment: production <9>
...
----
+
<1> 프라이빗 컨테이너 이미지 레지스트리의 주소
<2> 게이트웨이 부트스트랩의 포함 여부
<3> 네트워크 서비스 타입 
<4> 자체 서명 인증서의 사용 여부
<5> Route 53으로 생성한 도메인을 사용하기 위한 자동 인증서 관리 환경 사용 여부
<6> AWS에서 사용하는 계정의 액세스 키 ID
<7> 액세스 키 ID에 대한 시크릿 키
<8> Route 53으로 생성한 도메인에 대한 호스팅 영역 ID
<9> 실제 사용할 인증서 발급 용도
+
NOTE: 예시 외에 설치할 모듈에 대한 enabled 값을 true로 설정하거나, 필요시 사용자 지정 도메인을 등록한다.

. *shared-values.yaml 파일 수정*
+
`kubespray-aws/roles/bootstrap-cloud/templates/argocd_installer/application/helm/shared-values.yaml` 파일을 열어 마스터 클러스터에 필요한 구성 정보를 설정한다.
+
.예시
----
...
    repoURL: https://gitea.cloudqa.com/root/argocd-installer.git <1>
...
global:
  network:
    disabled: true <2>
  domain: cloudqa.com <3>
  keycloak:
    domain: hyperauth.cloudqa.com <4>
...
----
<1> ArgoCD와 연동된 Gitea 저장소 주소 (Gitea의 경우 url 마지막에 .git을 추가)
<2> 폐쇄망 환경 여부 (폐쇄망일 경우 true)
<3> 애플리케이션 설치 시 인그레스 주소에 사용될 커스텀 도메인 이름
<4> 설치할 HyperAuth 도메인 이름

. *애플리케이션 변수 설정*
+
`kubespray-aws/roles/bootstrap-cloud/templates/argocd_installer/application/app_of_apps/master-applications.yaml` 파일을 열어 마스터 클러스터의 애플리케이션 변수를 설정한다.
+
.예시
----
spec:
  ...
  source:
    ...
    repoURL: https://gitea.cloudqa.com/root/argocd-installer.git <1> 
    targetRevision: HEAD <2>
----
<1> ArgoCD와 연동된 Gitea 저장소 주소 (Gitea의 경우 url 마지막에 .git을 추가)
<2> Gitea에 연동되어 있는 argocd-installer의 브랜치 이름

. *Gitea 동기화 작업*
+
ArgoCD와 연동된 Gitea의 argocd-installer 브랜치에서 `master-values.yaml`, `shared-values.yaml`, `master-applications.yaml` 파일을 열어 위의 1~3번 과정과 동일하게 환경 변수를 설정한다.

. *애플리케이션 등록*
+
설치 환경에 애플리케이션을 등록한다.
+
----
$ kubectl -n argocd apply -f application/app_of_apps/master-applications.yaml
----

[#SingleClusterAws]
== 싱글 클러스터 설치

. *생성된 애플리케이션 파일 불러오기*
+
HyperCloud 웹 콘솔의 "멀티 클러스터" 콘솔에서 *[클러스터]* 메뉴를 클릭하면 싱글 클러스터 목록이 조회된다. 이때 싱글 클러스터 상태가 "Sync Needed"로 변경되면 해당 상태를 클릭한다.
+
image::../../images/figure_single_cluser_install_01.png[]

. *애플리케이션 변수 설정*
+
싱글 클러스터의 애플리케이션 동기화 옵션 설정 화면이 열리면 "REPO URL"과 "TARGET REVISION" 항목을 설정한다.
+
image::../../images/figure_single_cluser_install_03.png[]
+
설정 완료 후 애플리케이션의 *[SYNC]* 버튼을 클릭한다
+
image::../../images/figure_single_cluser_install_02.png[]

CAUTION: 클러스터 클레임으로 생성한 클러스터를 배포할 경우 대시보드를 통한 리소스 사용량 조회가 일부 지원되지 않는다.

[#ResourceDeployAws]
== 리소스 배포

애플리케이션 동기화 작업을 통해 리소스를 배포한다.

이때 마스터 클러스터와 싱글 클러스터에서 각각 동기화 작업을 진행해야 하며, 각 애플리케이션의 동기화 순서는 아래를 참고한다.

[CAUTION]
.마스터 클러스터 동기화 순서
====
마스터 클러스터에서 애플리케이션 동기화 순서는 다음과 같다. 반드시 순서에 맞게 동기화 작업을 수행한다. +
1. api-gateway-bootstrap(namespaces + cert-manager + jwt-decode-auth + api-gateway with console) +
2. gitea, argocd, hyperregistry +
3. strimzi-kafka-operator +
4. hyperauth +
5. loki +
6. prometheus +
7. grafana-operator +
8. service-mesh(istio, jaeger, kiali) +
9. hypercloud +
10. template-service-broker +
11. catalog-controller +
12. cicd-operator(tekton) +
13. cluster-api(aws, vsphere) +
14. redis-operator, helm-apiserver, service-binding-operator +
15. sonarqube, nexus +
16. image-validating-webhook
====

[CAUTION]
.싱글 클러스터 동기화 순서
====
싱글 클러스터에서 애플리케이션 동기화 순서는 다음과 같다. 반드시 순서에 맞게 동기화 작업을 수행한다. +
1. api-gateway-bootstrap(namespaces + cert-manager + jwt-decode-auth + api-gateway without console) + 
2. opensearch or loki +
3. prometheus +
4. grafana-operator +
5. hyperregistry +
6. service-mesh(istio, jaeger, kiali) + 
7. template-service-broker +
8. catalog-controller, cicd-operator(tekton), redis-operator, helm-apiserver, service-binding-operator +
9. sonarqube, nexus +
10. image-validating-webhook
====

. *ArgoCD 콘솔 접속*
+
웹 브라우저의 주소 표시줄에 ArgoCD 서버의 주소를 입력한다.
+
[NOTE]
====
ArgoCD 서버 주소는 다음의 명령을 실행하여 확인할 수 있다.
----
$ kubectl get ingress -n argocd
----
====

. *ArgoCD 콘솔 로그인*
+
ArgoCD 콘솔 로그인 화면이 열리면 계정 아이디와 비밀번호를 입력한 후 *[SIGN IN]* 버튼을 클릭한다.
+
[NOTE]
====
ArgoCD 계정 아이디 및 초기 비밀번호 정보는 다음의 명령을 실행하여 확인할 수 있다.
----
$ kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d; echo
----
ArgoCD 콘솔에 첫 로그인 시 위에서 확인한 계정 정보로 로그인이 가능하며, 로그인 후 *[User Info]* 메뉴를 통해 비밀번호를 변경할 수 있다.
====

. *동기화할 애플리케이션 검색*
+
ArgoCD 콘솔의 **Applications 화면**에서 동기화 작업을 수행할 애플리케이션을 검색한 후 *[SYNC]* 버튼을 클릭한다.
+
image::../../images/figure_application_sync_01.png[]

. *동기화 옵션 설정*
+
동기화할 리소스 및 동기화 옵션을 설정한 후 *[SYNCHRONIZE]* 버튼을 클릭한다.
+
image::../../images/figure_application_sync_02.png[]

. *상태 확인*
+
애플리케이션의 *Status* 항목에 "Healthy"와 "Synced"가 표시되는지 확인한다.
+
image::../../images/figure_application_sync_03.png[]
